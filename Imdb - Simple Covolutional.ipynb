{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB movie review sentiment sample\n",
    "\n",
    "Learn whether a movie review is positive or negative using the nutshell library.\n",
    "\n",
    "Modify the default LSTM model to use a 1D Convolutional network instead.\n",
    "\n",
    "Validation accuracy = 90.4%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nutshell import ModelData, Learner, TextReader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse movie review txt files into lists of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 25000 reviews\n"
     ]
    }
   ],
   "source": [
    "# read imdb movie review files into a list\n",
    "\n",
    "# download data from github - https://github.com/jalbertbowden/large-movie-reviews-dataset/tree/master/acl-imdb-v1\n",
    "# copy train & test subdirectories to this directory\n",
    "\n",
    "reader = TextReader()\n",
    "pos_texts = reader.read_text_files('./train/pos/*.txt')\n",
    "neg_texts = reader.read_text_files('./train/neg/*.txt')\n",
    "\n",
    "texts = pos_texts + neg_texts\n",
    "labels = ([1] * len(pos_texts)) + ([0] * len(neg_texts))\n",
    "\n",
    "# search and replace these values in each review\n",
    "# treat periods and commas like words and strip off some characters\n",
    "replacements = {'<br />': '', '\"': '', '(': '( ',')': ' )', \"'s \": \" 's \",\n",
    "                '?': ' ? ', '-': ' ', ', ': ' , ', '. ': ' . ', '*': ''}\n",
    "\n",
    "for i in range(0,len(texts)):\n",
    "    texts[i] = texts[i].lower()\n",
    "    texts[i] = reader.multi_replace(texts[i], replacements)\n",
    "        \n",
    "# parse review text into lists of words (delimited by \" \")\n",
    "\n",
    "word_lists = []\n",
    "for text in texts:\n",
    "    word_list = text.split(' ')\n",
    "    if len(word_list) > 1:\n",
    "        word_lists.append(word_list)\n",
    "\n",
    "print('Parsed', len(word_lists), 'reviews')   \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format data for building a simple LSTM for classification\n",
    "### - one that is able to predict whether the review sentiment is positive or negative\n",
    "\n",
    "- The single input is a list of word token ids\n",
    " - The words in the review were tokenized in the prepare_data \n",
    "- The label is a 1 for positive and 0 for negative\n",
    "- The model will output a floating point number between 0 and 1\n",
    " - Values >= .5 can be considered positive reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing category columns...\n",
      "words 153820 unique values\n",
      "Done preparing data\n"
     ]
    }
   ],
   "source": [
    "dfInput = pd.DataFrame()\n",
    "dfInput['words'] = word_lists\n",
    "dfInput['label'] = labels\n",
    "data = ModelData(dfInput)\n",
    "data.category_columns = ['words'] # indicates the contents are categories, not numeric values\n",
    "data.sequence_columns = ['words'] # indicates the column contains a list of category values\n",
    "data.label_column = 'label'\n",
    "data.sequence_length = 1500 # almost all reviews are < 1000 words\n",
    "data.validation_split = .10 \n",
    "data.prepare_data()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training examples: 22500\n",
      "Validation examples: 2500\n"
     ]
    }
   ],
   "source": [
    "data.split_data(shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keras Model\n",
    "\n",
    "Learner object will choose LSTM/Dropout layer sets for the sequential inputs.\n",
    "\n",
    "After the default model is built, modify the model to use a 1D convolutional network instead of the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential Merge Layer Shape:  (?, 1500, 50)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_words (InputLayer)     (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "embed_words (Embedding)      (None, 1500, 50)          7691200   \n",
      "_________________________________________________________________\n",
      "lstm_0 (LSTM)                (None, 1500, 24)          7200      \n",
      "_________________________________________________________________\n",
      "lstm_dropout_0 (Dropout)     (None, 1500, 24)          0         \n",
      "_________________________________________________________________\n",
      "lstm_timedist (TimeDistribut (None, 1500, 24)          600       \n",
      "_________________________________________________________________\n",
      "lstm_reshape (Reshape)       (None, 36000)             0         \n",
      "_________________________________________________________________\n",
      "dense_representation (Dense) (None, 50)                1800050   \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 9,499,101\n",
      "Trainable params: 9,499,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# build model\n",
    "learner = Learner(data)\n",
    "learner.hidden_layers = 1 # number of lstm/dropout layer pairs\n",
    "learner.dropout_rate = .30\n",
    "learner.batch_size = 256\n",
    "learner.gpu = True\n",
    "learner.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_words (InputLayer)     (None, 1500)              0         \n",
      "_________________________________________________________________\n",
      "embed_words (Embedding)      (None, 1500, 50)          7691200   \n",
      "_________________________________________________________________\n",
      "conv_0 (Conv1D)              (None, 1496, 256)         64256     \n",
      "_________________________________________________________________\n",
      "conv_maxpool_0 (Lambda)      (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "conv_dense_0 (Dense)         (None, 256)               65792     \n",
      "_________________________________________________________________\n",
      "dropout_30 (Dropout)         (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_representation (Dense) (None, 50)                12850     \n",
      "_________________________________________________________________\n",
      "dense_output (Dense)         (None, 1)                 51        \n",
      "=================================================================\n",
      "Total params: 7,834,149\n",
      "Trainable params: 7,834,149\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Replace default LSTM layers with a convolutional layer\n",
    "# Also add dropout to embedding layer\n",
    "# Re-compile model\n",
    "\n",
    "# You can just replace the default model with a new model, \n",
    "#  but I want to show that you can add to the existing model also\n",
    "\n",
    "# base on this thread: https://github.com/keras-team/keras/issues/2296\n",
    "\n",
    "from keras.layers import Dense, Dropout, Conv1D, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "\n",
    "def max1d(X):\n",
    "    return K.max(X, axis=1)\n",
    "\n",
    "filters = 256\n",
    "filter_size = 5\n",
    "\n",
    "# add dropout to embedding layer\n",
    "learner.model.get_layer('embed_words').dropout=.20\n",
    "\n",
    "# reference to cut and paste point\n",
    "embed_words = learner.model.get_layer('embed_words').output\n",
    "\n",
    "# define new layers - attach to references\n",
    "x = Conv1D(filters, filter_size, strides=1, \\\n",
    "                    padding='valid', activation='relu', name='conv_0')(embed_words)\n",
    "x = Lambda(max1d, output_shape=(filters,), name='conv_maxpool_0')(x)\n",
    "x = Dense(filters, name='conv_dense_0')(x)\n",
    "x = Dropout(learner.dropout_rate)(x)\n",
    "x = Dense(learner.output_factors, name='dense_representation')(x)\n",
    "output = Dense(1, name='dense_output')(x)\n",
    "\n",
    "new_model = Model(inputs=[learner.model.get_layer('input_words').input], \\\n",
    "                  outputs=output)\n",
    "new_model.compile(loss='mse', optimizer=Adam(), metrics=['acc'] )\n",
    "learner.model = new_model\n",
    "\n",
    "print(learner.model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Epoch: 1\n",
      "Learning Rate: 0.001\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/2\n",
      "22500/22500 [==============================] - 20s 895us/step - loss: 0.2168 - acc: 0.6486 - val_loss: 0.1255 - val_acc: 0.8524\n",
      "Epoch 2/2\n",
      "22500/22500 [==============================] - 18s 790us/step - loss: 0.0857 - acc: 0.9147 - val_loss: 0.0949 - val_acc: 0.8948\n"
     ]
    }
   ],
   "source": [
    "learner.train_model(filename='imdb_conv', epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Epoch: 1\n",
      "Learning Rate: 0.0001\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "22500/22500 [==============================] - 18s 791us/step - loss: 0.0272 - acc: 0.9940 - val_loss: 0.0938 - val_acc: 0.9028\n"
     ]
    }
   ],
   "source": [
    "learner.train_model(filename='imdb_conv', learning_rate=.0001, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Epoch: 1\n",
      "Learning Rate: 1e-05\n",
      "Train on 22500 samples, validate on 2500 samples\n",
      "Epoch 1/1\n",
      "22500/22500 [==============================] - 18s 794us/step - loss: 0.0166 - acc: 0.9987 - val_loss: 0.0903 - val_acc: 0.9048\n"
     ]
    }
   ],
   "source": [
    "learner.train_model(filename='imdb_conv', learning_rate=.00001, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 conv - 1000 len/256x3filt/.30drop/.20embdrop - ep3/1/1 = .8908 valacc\n",
    "#1 conv - 1000/256x5/.30/.20 - ep2 = .8944 \n",
    "#2 conv - 1000/500x3/.3/.2 - - ep3/1 = .8940\n",
    "#3 conv - 1000/128x5/.3/.2 - ep3/3/3 = .8840\n",
    "#4 conv - 1500/256x5/.3/.2 - ep3/2/2 = .9040 #best"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
