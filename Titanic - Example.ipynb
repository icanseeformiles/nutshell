{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nutshell import ModelData, Learner, Predictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing category columns...\n",
      "Survived 2\n",
      "Pclass 3\n",
      "Sex 2\n",
      "Embarked 4\n",
      "Imputing and scaling numeric columns...\n",
      "Age\n",
      "SibSp\n",
      "Parch\n",
      "Fare\n",
      "Done preparing data\n",
      "Training examples: 802\n",
      "Validation examples: 89\n"
     ]
    }
   ],
   "source": [
    "data = ModelData(pd.read_csv('titanic_train.csv'))\n",
    "data.category_columns = ['Survived', 'Pclass', 'Sex', 'Embarked']\n",
    "data.numeric_columns = ['Age', 'SibSp', 'Parch', 'Fare']\n",
    "data.label_column = 'Survived'\n",
    "data.key_column = 'PassengerId'\n",
    "data.prepare_data()\n",
    "data.validation_split = .10\n",
    "data.split_data(shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non-Sequential Merge Layer Shape: (?, 11)\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_Pclass (InputLayer)        (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_Sex (InputLayer)           (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_Embarked (InputLayer)      (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "embed_Pclass (Embedding)         (None, 1, 2)          10          input_Pclass[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "embed_Sex (Embedding)            (None, 1, 2)          8           input_Sex[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "embed_Embarked (Embedding)       (None, 1, 3)          18          input_Embarked[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "reshape_Pclass (Reshape)         (None, 2)             0           embed_Pclass[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "reshape_Sex (Reshape)            (None, 2)             0           embed_Sex[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "reshape_Embarked (Reshape)       (None, 3)             0           embed_Embarked[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "input_Age (InputLayer)           (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_SibSp (InputLayer)         (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_Parch (InputLayer)         (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "input_Fare (InputLayer)          (None, 1)             0                                            \n",
      "____________________________________________________________________________________________________\n",
      "merge_non_sequential (Concatenat (None, 11)            0           reshape_Pclass[0][0]             \n",
      "                                                                   reshape_Sex[0][0]                \n",
      "                                                                   reshape_Embarked[0][0]           \n",
      "                                                                   input_Age[0][0]                  \n",
      "                                                                   input_SibSp[0][0]                \n",
      "                                                                   input_Parch[0][0]                \n",
      "                                                                   input_Fare[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_0 (Dense)                  (None, 50)            600         merge_non_sequential[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dense_dropout_0 (Dropout)        (None, 50)            0           dense_0[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_reshape (Reshape)          (None, 50)            0           dense_dropout_0[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_representation (Dense)     (None, 50)            2550        dense_reshape[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_output (Dense)             (None, 1)             51          dense_representation[0][0]       \n",
      "====================================================================================================\n",
      "Total params: 3,237\n",
      "Trainable params: 3,237\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "learner = Learner(data)\n",
    "learner.label_type = 'binary'\n",
    "learner.hidden_layers = 1\n",
    "learner.dropout_rate = .30\n",
    "learner.build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Super Epoch: 1\n",
      "Learning Rate: 0.001\n",
      "Train on 802 samples, validate on 89 samples\n",
      "Epoch 1/10\n",
      "802/802 [==============================] - 0s - loss: 0.3501 - acc: 0.5973 - val_loss: 0.2584 - val_acc: 0.6180\n",
      "Epoch 2/10\n",
      "802/802 [==============================] - 0s - loss: 0.2661 - acc: 0.6409 - val_loss: 0.2486 - val_acc: 0.6180\n",
      "Epoch 3/10\n",
      "802/802 [==============================] - 0s - loss: 0.2418 - acc: 0.6608 - val_loss: 0.2129 - val_acc: 0.6742\n",
      "Epoch 4/10\n",
      "802/802 [==============================] - 0s - loss: 0.2269 - acc: 0.6646 - val_loss: 0.2147 - val_acc: 0.6404\n",
      "Epoch 5/10\n",
      "802/802 [==============================] - 0s - loss: 0.1925 - acc: 0.7195 - val_loss: 0.1942 - val_acc: 0.6742\n",
      "Epoch 6/10\n",
      "802/802 [==============================] - 0s - loss: 0.1731 - acc: 0.7581 - val_loss: 0.1732 - val_acc: 0.7191\n",
      "Epoch 7/10\n",
      "802/802 [==============================] - 0s - loss: 0.1702 - acc: 0.7581 - val_loss: 0.1719 - val_acc: 0.7640\n",
      "Epoch 8/10\n",
      "802/802 [==============================] - 0s - loss: 0.1571 - acc: 0.7918 - val_loss: 0.1687 - val_acc: 0.7753\n",
      "Epoch 9/10\n",
      "802/802 [==============================] - 0s - loss: 0.1553 - acc: 0.8067 - val_loss: 0.1708 - val_acc: 0.7640\n",
      "Epoch 10/10\n",
      "802/802 [==============================] - 0s - loss: 0.1655 - acc: 0.7880 - val_loss: 0.1683 - val_acc: 0.7753\n",
      "Super Epoch: 2\n",
      "Learning Rate: 0.0001\n",
      "Train on 802 samples, validate on 89 samples\n",
      "Epoch 1/10\n",
      "802/802 [==============================] - 0s - loss: 0.1603 - acc: 0.7893 - val_loss: 0.1730 - val_acc: 0.7753\n",
      "Epoch 2/10\n",
      "802/802 [==============================] - 0s - loss: 0.1523 - acc: 0.7980 - val_loss: 0.1742 - val_acc: 0.7865\n",
      "Epoch 3/10\n",
      "802/802 [==============================] - 0s - loss: 0.1527 - acc: 0.8055 - val_loss: 0.1704 - val_acc: 0.7528\n",
      "Epoch 4/10\n",
      "802/802 [==============================] - 0s - loss: 0.1739 - acc: 0.7631 - val_loss: 0.1713 - val_acc: 0.7191\n",
      "Epoch 5/10\n",
      "802/802 [==============================] - 0s - loss: 0.1521 - acc: 0.8042 - val_loss: 0.1612 - val_acc: 0.7865\n",
      "Epoch 6/10\n",
      "802/802 [==============================] - 0s - loss: 0.1491 - acc: 0.8067 - val_loss: 0.1659 - val_acc: 0.7753\n",
      "Epoch 7/10\n",
      "802/802 [==============================] - 0s - loss: 0.1429 - acc: 0.8142 - val_loss: 0.1614 - val_acc: 0.7865\n",
      "Epoch 8/10\n",
      "802/802 [==============================] - 0s - loss: 0.1416 - acc: 0.8130 - val_loss: 0.1625 - val_acc: 0.7865\n",
      "Super Epoch: 3\n",
      "Learning Rate: 1e-05\n",
      "Train on 802 samples, validate on 89 samples\n",
      "Epoch 1/10\n",
      "802/802 [==============================] - 0s - loss: 0.1383 - acc: 0.8192 - val_loss: 0.1649 - val_acc: 0.7753\n",
      "Epoch 2/10\n",
      "802/802 [==============================] - 0s - loss: 0.1402 - acc: 0.8130 - val_loss: 0.1615 - val_acc: 0.7865\n",
      "Epoch 3/10\n",
      "802/802 [==============================] - 0s - loss: 0.1406 - acc: 0.8242 - val_loss: 0.1594 - val_acc: 0.7865\n",
      "Epoch 4/10\n",
      "802/802 [==============================] - 0s - loss: 0.1388 - acc: 0.8279 - val_loss: 0.1623 - val_acc: 0.7865\n",
      "Epoch 5/10\n",
      "802/802 [==============================] - 0s - loss: 0.1424 - acc: 0.8092 - val_loss: 0.1634 - val_acc: 0.7865\n",
      "Epoch 6/10\n",
      "802/802 [==============================] - 0s - loss: 0.1415 - acc: 0.8155 - val_loss: 0.1676 - val_acc: 0.7865\n",
      "Super Epoch: 4\n",
      "Learning Rate: 1.0000000000000002e-06\n",
      "Train on 802 samples, validate on 89 samples\n",
      "Epoch 1/10\n",
      "802/802 [==============================] - 0s - loss: 0.1404 - acc: 0.8180 - val_loss: 0.1593 - val_acc: 0.7753\n",
      "Epoch 2/10\n",
      "802/802 [==============================] - 0s - loss: 0.1351 - acc: 0.8279 - val_loss: 0.1583 - val_acc: 0.7753\n",
      "Epoch 3/10\n",
      "802/802 [==============================] - 0s - loss: 0.1302 - acc: 0.8317 - val_loss: 0.1609 - val_acc: 0.7753\n",
      "Epoch 4/10\n",
      "802/802 [==============================] - 0s - loss: 0.1374 - acc: 0.8254 - val_loss: 0.1591 - val_acc: 0.7753\n",
      "Epoch 5/10\n",
      "802/802 [==============================] - 0s - loss: 0.1392 - acc: 0.8242 - val_loss: 0.1624 - val_acc: 0.7640\n"
     ]
    }
   ],
   "source": [
    "learner.train_model(filename='titanic', epochs=10, super_epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use trained model to score test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing category columns...\n",
      "** Using pre-defined token map **\n",
      "Pclass 3\n",
      "Sex 2\n",
      "Embarked 4\n",
      "Imputing and scaling numeric columns...\n",
      "** Using pre-defined impute/scale metadata **\n",
      "Age\n",
      "SibSp\n",
      "Parch\n",
      "Fare\n",
      "Done preparing data\n"
     ]
    }
   ],
   "source": [
    "# now use the trained model and make predictions on test data\n",
    "test_data = ModelData(pd.read_csv('titanic_test.csv'), settings_filename = 'titanic_settings')\n",
    "test_data.prepare_data()\n",
    "predictor = Predictor('titanic_model', test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 32/418 [=>............................] - ETA: 1s\n",
      "Done scoring\n",
      "   PassengerId     score  Survived\n",
      "0          892  0.169405         0\n",
      "1          893  0.587477         1\n",
      "2          894  0.217067         0\n",
      "3          895  0.177683         0\n",
      "4          896  0.502400         1\n",
      "5          897  0.243897         0\n",
      "6          898  0.702010         1\n",
      "7          899  0.354489         0\n",
      "8          900  0.713587         1\n",
      "9          901  0.146013         0\n"
     ]
    }
   ],
   "source": [
    "# make predictions on test data\n",
    "predictor.score()\n",
    "\n",
    "# convert probability score to a 0 or 1 for submission\n",
    "predictor.modeldata.prep_data['Survived'] = predictor.modeldata.prep_data['score'].apply(lambda x: 0 if x<.5 else 1)\n",
    "\n",
    "print(predictor.modeldata.prep_data[['PassengerId', 'score', 'Survived']][0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write submission file\n",
    "predictor.modeldata.write_csv(['PassengerId', 'Survived'], 'titanic_submission1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
